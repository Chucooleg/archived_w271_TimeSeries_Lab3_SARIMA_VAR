To do Now:

388 (do diff.diff12)

(autosales diff.diff12)

517 (redo auto sales conclusion)


Unemployment series stationarity conclusion: Regarding first differencing, all of our plots and tests suggests strong random walk behavior with the raw series, so it is recommended. Regarding additional seasonal differencing, the results were contradictory. The monthly, acf and pacf plots suggest noticeable seasonal pattern, while the scatterplot matrix doesn't. The unit test suggested that either first or seasonal differencing can help us to establish a stationary series. At this stage, we remain open to a first differenced, or first differenced in additional to seasonally differenced model. We defer to the modeling and forecasting results to determine the best candidate.


The monthly plot and autocorrelation plots exhibit seasonal behaviors


EDA

1. Data Overview (DONE)
unem has 336 more obs than auto

2. Time Series Overview

### Time plots and Histograms
unem.ts : persistency, unusual spikes (due to recession)
auto.ts : less persistent than unem. stronger seasonal jots. two plunges corresponding to unem.ts recessions. spike in 1986 due to oil prices drop


(persistent)
(clear outlier, examine) (continuous from persistent trend not errorneous, correspond to the early 1980s recession in the US which ended in November 1982). second peak correspond to the Great recession which officially lasted from Dec 2007 - June 2009)


From the above time plot of unemployment, we see clear persistency of the observations. That is, when observations are above or below the mean, they tend to stay so for a while. The overall trend seem to climb slowly upward. There are a noticeable number of observations lying 2.5 standard deviations above the mean, which is made obvious in the right skewed histogram as well. We isolated these observations, the 6 in 1982 and 1983 probably correspond to the early 1980s recessions which officially ended in November 1982. The 3 in 2010 probably correspond to the late 2000s recession which officially ended in June 2009.

(A Sharp Spike in 1980s)
(less persistent than employment rate)
(two plunges in the early 1980s and 2009 respond to recessions)
(one spike in 1986 due to oil prices plunged in more than half that year)

From the above time plot of auto sales, we see some but weaker persistency than the unemployment series. The overall trend doesn't seem to climb upward or downward. There are more noticeable seasonal patterns than the unemployment series. The histogram is more symmetric and normal. Less than 1.5% observations lie 2 standard deviations above the mean and less than 2.5% observations lie 2 standard deviations below the mean.  We isolated 
these observations, the 5 in 1982 and 1983 and the 5 in 2008 and 2009 probably correspond to the two aforementioned recessions. Notice that these recession related observations in auto sales tend to happen a few months before those in unemployment. There is an unusual spike in 1986, probably attributed to oil prices dropping in half that year.


### Smoothing and Decomposition 181

Unem MA and kernel:

The asymmetric moving average filters above average over the past 6 months, 12 months and 4 years. The symmetric kernel smoothers attempted 3 bandwidths. In either case, the smoothed series resemble behavior of random walks with drift, evidence by the gradually widened variance and slowly increasing mean towards the right. Formulation for the two smoothers are given here:

Unem Decomposition :

The decomposed trend series resembles that of the moving average and kernel filters. Here the growing variance of the trend is more noticeable, as well as the gradual upward trend. A regular, annual seasonality series was isolated out from the raw series. The random component series is clearly non-stationary. Its variance diminishes over time featured by dramatic spikes before 1965. Clearly, an OLS model would not be the right choice, we should not disregard the non-stationary trend and random components.

Auto MA and kernel:

Compared to the unemployment series, the auto sale series exhibit less downhill-uphill-downhill behavior, and appear to be relatively stable between the two aforementioned recessions in early 1980s and late 2000s. It resembles less of a random walk and drift is not entirely apparent. Seasonal pattern is also stronger. Below, we plotted the two series at the same time interval and kernel bandwidth for direct comparison.

Auto Decomposition :

(Decomposition Assumption, Findings of Noise of Inconsistent Variance)
(Trend is sloght upward trend, doesn't look like a random walk. Compared to unemployment it is more stable)
(Seasonality seem regular)
(Noise is clearly not stationary)
(We cannot use regression mode)

The trend component observed in the decomposed series concur with our intuitions from the smoothed series. With the seasonal component taken out, the random component show a spike in mid-1980s and several more in 2000s, both of which may correspond to respective oil price fluctuations. In general, the random component series shows higher variance before mid-1980s than after, which we have also observed in the unemployment series. The decomposed components are clearly not stationary, which against suggest that OLS is not appropriate. 



3. Establish Stationarity 299


## Seasonality and Autocorrelation -- Unem

month plot 304

(monthly plot shows higher values in the beginning and mid year)
(can difference it in SARIMA, if linear regression should use dummy for Jan Feb and Jun July)

The monthly plot show some seasonal pattern at the mean, but the range of unemployment variation for each month also overlap a lot with other months. So the seasonal pattern exists but is not very strong. We see that unemployment rate tend to be a little higher in the beginning and middle of each year followed by mild gradual decrease.

scatterplot 310

(weak seasonality, probably no need for seasonal difference)
(strongest correlation with first lag, then weaker and weaker lags)

The above scatterplot shows that the series is most correlated with its first lag, then correlations grow weaker and the point clouds becomes more scattered with larger lags without any sign of picking up at lag 12. 

(weak seasonality, probably no need for seasonal difference)
(strongest correlation with first lag, then weaker and weaker lags)


acf, pacf 316

(acf slow decay, random walk with drift)
(some sign of seasonality -- note the local peak at lag 12, 24, 36, 48)

(pacf most significant at lag 1 can be AR(1))
(pacf also show sign at lag 12, 25,37,49 , can be a seasonal MA component)

The acf shows slow decay from lag 0 coupled with a sharp drop of pacf after lag 1. This is a sign of an AR(1) process. We see minor local ripples on the acf at lag 12, 24, 36 and 48 which indicate seasonal effects. The significant, negative pacfs which tails off at lag 13, 25, 37, 49 suggest a seasonal MA component. Some smaller significant pacf values within the first 12 lags suggest either some AR processes or an MA process in that range.

Although there are some seasonal effects in the unemployment series, they don't seem strong enough to require seasonal differencing. To help us determine, we compare the seasonal differenced and first differenced series and compare their behaviors using time and autocorrelation plots.

(TS plot 1st diff, 12th diff) 358

The seasonally differenced series retained most of the random walk like behavior in the raw series and appear more persistent before mid 1980s. The first differenced series eliminated most random walk and persistent behaviors but variance is generally larger before 1980. There is still a slight downward trend (green regression line) with the seasonal differenced series. (red line refers to 2 standard deviation marks)

(acf, pacf of 12th diff) 367

(pacf sees sign lags 13, 25, 37 without much echo of graudal positive decay in acf --> seasonal MA(1) )
(Seasonal Differencing may not be necessary --> Seasonal MA(1) can be a sign that we have over differenced the series)
(pacf sharp drop lag 1, echoed by acf gradual drop from lag 1, AR(1))
(pacf some sign lags 2-4, can be AR(2-4) components, hard to tell from acf, let model loop judge)

Above plots: In the seasonal differenced series, we still see a significant pacf at lag 1 followed by a sharp drop and gradual decay in the acf. Notice the significant pacfs in lags 13, 25, 37 as well. This suggest AR(1) process in combination with seasonal AR(3), or in combination with some seasonal ARMA processes. Notice a couple significant pacfs between lag 2 to 4, they can be AR(2-4) components but hard to tell at this stage. We may entertain the model as a SARIMA(4,0,q)(3,1,Q) using these plots.

(acf, pacf of 1st diff) 373

(long memory seasonal AR(1) and seasonal MA(1))
(pacf show some significance lag 2-9 , without much echo in acf --> let loop figure it out)
possible SARIMA(9,1,q)(1,0,1)

Above plots: In the first differenced series, the plots show much stronger periodic behavior, the extended plot show that acf tails off until lag 720, which suggest either a seasonal long memory or seasonal integrated process (both SAR(1)) in combination with some sesonal AR(2-4) components or in combination with a seasonal MA component. The pacf show some significance between lag 2-9 without a little echo in the acf lag 13-16, which can come from some MA processes but it's hard to say at this stage. We simply assume some ARMA processes present. We may entertain the model as a SARIMA(9,1,q)(4,0,Q) using these plots.

(acf, pacf of 1st and 12th diff) 396

Above plots: In the first and seasonal differenced series, the autocorrelation plots doesn't show an AR(1) component anymore, we still see a significant acf at lag 12 and significant decaying pacf at lag 12, 24, 36 and 48, which suggests a seasonal MA(1) process. At lag 2-4, the pacf are still significant with some echos in acf which suggests some AR(4) components. We simply assume some ARMA processes present. We may entertain the model as a SARIMA(4,1,0)(0,1,1) using these plots.


Unit root tests 400

We perform unit root tests below to check for stationarity. Augmented Dicky Fuller Test and Phillips Perron Tests are performed, with the following test hypotheses:

Specifically in the ADF test, our null hypothesis assume that the process is a random walk with drift and some AR(p) components,$x_t = \beta_0 + \phi x_{t-1} + \sum_{j=1}^{p-1}\psi_j x_{t-j}  + w_t$, where $\beta_0$ represents the drift and $\phi = 1$. We are essentially testing the null hypothesis $\gamma = 0$ in the differenced series $\nabla x_t = \gamma x_{t-1} +  \sum_{j=1}^{p-1}\psi_j \nabla x_{t-j}  + w_t$ since $\gamma = \phi - 1$. Under the alternative hypothesis $\gamma < 0$.

In the Philips Perron test, our null hypothesis assume a model $x_t = \beta_0 + \rho_1 x_{t-1} + u_t$, where non-parametric correction is applied on $\rho$ to correct for serial correlation in $u_t$ already. We are essential testing of $\rho = 1$ in the null hypothesis.

As expected with the raw series, the ADF test failed to reject the null hypothesis that the series contains a unit root. The PP test results contradicts but we acknowledge that the test mechanism is different and that some early literature (Davidson and Mackinnon 2004) showed that ADF performs better in finite sample than PP test. All tests on both the seasonal differenced and first differenced series rejected the null hypothesis to support stationarity of the series.

All the tests rejected the null hypothesis that the first differenced or seasonal differenced series contains a unit root and supports stationarity. We should not need to difference the raw series with both procedures.

Unem Conclusion 418

Unemployment series stationarity conclusion: Regarding first differencing, all of our plots and tests suggests strong random walk behavior with the raw series, so it is recommended. Regarding additional seasonal differencing, the results were contradictory. The monthly, acf and pacf plots suggest noticeable seasonal pattern, while the scatterplot matrix doesn't. The unit test suggested that either first or seasonal differencing can help us to establish a stationary series. At this stage, we remain open to a first differenced, or first differenced in additional to seasonally differenced model. We defer to the modeling and forecasting results to determine the best candidate.



## Seasonality and Autocorrelation -- Auto

Month Plot 425

(monthly plot shows more discrete values)

Compared to that of unemployment rate, monthly plot of auto sales show much more discrete values in the mean. So the seasonal pattern is much stronger. We see that autosales tend to be noticeably lower in the first two months, picks up sharply in March, steps up and down for the rest of the year. Fall sales are generally lower than summer sales.

Scatterplot 431 

(seasonality is much stronger. clearly correlated with lag 12)

Scatterplot matrix of auto sales with its own lags display clear seasonal dependence. Autocorrelation is highest with lag 12, that is this month last year, even more so compared to lag 1. The opposite patterns was observed in the unemployment rates series, when autocorrelation is highest with lag 1 and then gradually declines towards higher lags.

acf, pacf 438

(acf slow decay but show clearn local maximum at lag 12, 24, 36 , 48)
(pacf show sign at lag 1 and 12, AR(1) and SAR(1))

Similar to the unemployment series, the acf shows slow decay from lag 0, a sharp drop of pacf after lag 1 and some local acf maximums at lag 12, 24, 36 and 48. Unlike the unemployment series, the acf decay ends earlier at lag 36. The local maxima have much stronger profiles. Some significant pacf values occur before lag 12 which suggest either some AR or MA components(effects on acf plot is not discernable) in that range.

Based strong seasonal pattern in the above three plots, seasonal differencing would be useful in achieving stationarity. To help us determine, we again compare the seasonal differenced and first differenced series and compare their behaviors using time and autocorrelation plots.

TS plots (diff 1 diff 12) 479 

The seasonally differenced series noticeably removed the strong seasonal patterns and some degree of persistency from the raw series. The first differenced series removed most persistencies from the raw series but appear clustered regularly at seasonally intervals. Both series removed the upward trend in the raw series effectively.

(acf, pacf of 12th diff) 489

(pacf Still see -ve autocorrelation on Lag 1,2 and 3, acf after lag 3 shows gradual decay --> can be seasonal AR(3))
(pacf still see sharp drop at lag 1 and acf still see graudual decay from lag 1)
(pacf see sign, gradual drop lag 2-5, can be a MA(1) as well)

Above plots: Similar to unmployment rate, in the seasonal differenced serie of auto sales, we still see a significant pacf at lag 1 followed by a sharp drop and gradual decay in the acf. Notice the significant pacfs in lag 12, 24 and 36. This suggests an AR(1) process in combination with seasonal AR(3). Also notice a few significant pacfs at lags 2-4. We may entertain the model as a SARIMA(4,0,q)(3,1,Q) using these plots.

(acf, pacf of 1st diff) 543

Above plots: Similar to unmployment rate, in the first differenced serie of auto sales, the acf tails off very slowly until lag 360. Unlike that of the unemployment series, the seasonal ripples on the acf appear much stronger. This plots suggest a strong seasonal AR(1) component and some AR(p) components before lag 12. 

(acf, pacf of 1st 12th diff) 520

Above plots: In the first and seasonal differenced series, the autocorrelation plots doesn't show an AR(1) component anymore, we still see significant acf and pacf around lag 12, 24, 36, which indicate some seasonal ARMA components. We also see a significant acf at lag 1 echoed by some significant pacf which indicate an MA(1) component. We may entertain the model as a SARIMA(1,1,0)(P,1,Q) using these plots. Notice that the occurence of MA components could come from over-differencing the raw series. But if our goal is to produce better forecast rather than estimating a random component detrended from the raw series, first in additional to seasonal differencing is still acceptable.

Unit root tests 511

We perform unit root tests below to check for stationarity again. Augmented Dicky Fuller Test and Phillips Perron Tests are performed, with the following test hypotheses:

All tests rejected the null hypotheses for all four series. Notice that the ADF test p-value for the raw series is closer to the critical cut off of 0.05. There can be a very weak chance that the raw series contains a unit root. We examine the first difference series.

Auto stationarity conclusion 542

Auto sales series stationarity conclusion: Regarding seasonal differencing, all of our plots suggests strong seasonal patterns in the raw series, so it is recommended. Regarding first differencing, our raw series and seasonally difference series both show noticeable AR(1) behavior and the unit test only weakly reject the null hypothesis of stationarity. At this stage, we remain open to a seasonal difference, or first differenced in addition to seasonal differenced model. Again, we defer to the modeling and forecasting results to determine the best candidate.



### Examine month plot (aggregate by month)

### Scatterplot matrix with itself

Seasonal Differencing? Identify Seasonality

- Examine acf and pacf

- test for unit root (adf test, pp test)

- Difference it once

- plot the differenced ts

First Differencing? Identify if random walk or random walk with drift

- Examine acf and pacf from seasonally differenced ts

- test for unit root again (adf test, pp test)

- Difference it once more

- plot the differenced ts

- Look stationary? (Ljung-Box test)

- Examine acf and pacf

- eyeball AR, MA components
--------------------------------------------------------------------------------------------


562

4. Examine Bivariate Relationship

- intersect the two time series

### ts Plot overlap two series 580

It appears that between 1976 to early 1990s and between late 2000s and 2017, a crest in auto sales would be echoed with a trough in unemployment rate a few months later and a trough in auto sales would be echoed by a crest in unemployment rate a few months later. The unemployment series has a slight downward trend over the whole time interval while the opposite is true for auto sales.

### Scatterplot matrix 627

Disregarding time-dependent variations, the two series show some non-linear relationship in addition to overall negative correlation. The annually aggregated series show stronger and more linear correlation. It seems that elimination of seasonal granularity may "hide" otherwise non-linear relationships, which can be depicted by the scatterplot matrix below. Correlation of unemployment series against auto sales series remains moderate for more than 12 lags, and it peaks at lag 5 and 6. 

### ccf plot

The ccf plot below assess the cross-correlation $$,

### Scatterplot matrix with each other

### Did we get unit-roots above?
--Yes --> test for co-integration (po.test) (VECM, or difference both and do VAR)
--Yes --> try to difference the two and examine the linear combo as a ts, acf and pacf

### Take first difference and suggest VAR



Q3. 
W10 Live Notes --> Why SARIMA may be better than VAR.
-------------------------------------------------------------

Take away

How to VAR?
auto clearly lags behind unem --> doesn't make much sense to use auto info
don't strongly suggest unit root in both TS
cointegration test yes
but regressing unem on auto is not informative, residuals behavior are poor. linear combo is not strong

Bivariate investigation conclusion



-------------------------------------------------------------

Q1.

(Sue)
Generally speaking, for a model with both trend and seasonality, both seasonal difference and non-seasonal difference should be applied to get stationary series.  Suppose that S is the number of time periods during a season, e.g. S=12 for monthly data and S=4 for quarterly data. Take a seasonal difference of lag S, which means take the Sth difference to get rid of seasonality. In this case the trend is linear, so take first difference as non-seasonal differencing to remove trend for data.

(Michelle)
It would be necessary to difference once with lag 1 to remove the linear trend and once with the appropriate seasonal lag (likely 12 in this case) to remove the seasonality. This is because...?

(Legg)

To remove linear trend it is necessary to difference once with lag 1. Because in a purely linear trend, differencing by the last observation singles out the roughly equivalent increments which are stationary at the mean. To remove seasonal patterns, it is necessary to difference once with lag 12 for annual patterns, or lag 4 for quarterly patterns. Because in a purely seasonal series where each observation is same as the one from the last season, differencing by the observation from last season removes seasonal fluctuations and stablize the series at the mean. Real world economic time series, like ours, often exhibit both linear trend in additional to annual seasonal behavior. Therefore differencing by lag 1 and lag 12 will help stablize the raw series at the mean. In the following we simulate a raw series that is a random walk with drift(linear trend) and strong dependence on its seasonal lag 1, then demonstrate how first and seasonal differencing can stablize it at the mean. The model is :

-------------------------------------------------------------
Q2.

For seasonal differenced models, we reduced from the top AIC candidate, SARIMA(4,0,3)(0,1,1) to 7th candidate SARIMA(2,0,1)(0,1,1) and retain white noise residual behavior. 8th candidate SARIMA(1,0,3)(2,1,3) starts to show significant pacfs at lag 4 and 5, therefore we stop searching from there downwards. We will keep these two candidates to compare out of sample performance:

	- SARIMA(4,0,3)(0,1,1)
	- SARIMA(2,0,1)(0,1,1)

For first differenced models, we reduced from the top AIC candidate, SARIMA(8,1,3)(1,0,1) to 3rd candidate SARIMA(2,1,1)(1,0,1) and retain white noise residual behavior. 6th candidate SARIMA(1,1,1)(1,0,1) is still satisfactory but more of its lags are slightly closer to the cut-off. 7th candidate SARIMA(1,1,0)(1,0,1) starts to show significant pacfs at lag 2, 3 and 5, and 8th candidate SARIMA(1,1,0)(1,0,0) starts to show significant pacfs at seasonal lags, therefore we stop searching from there downwards. We will keep the first three candidates to compare out of sample performance:

	- SARIMA(8,1,3)(1,0,1)
	- SARIMA(2,1,1)(1,0,1)
	- SARIMA(1,1,1)(1,0,1)
 
For first and seasonal differenced models, we reduced from the top AIC candidate, SARIMA(2,1,3)(1,1,1) to 2nd candidate SARIMA(2,1,1)(0,1,1) and 3rd candidate(1,1,2)(3,1,3) with similar white noise behaviors. The 4th candidate SARIMA(1,1,2)(0,1,1), 5th candidate SARIMA(1,1,1)(3,1,3) and 6th SARIMA(1,1,1)(0,1,1) are still satisfactory but more of their lags are slightly closer to the cut-off. 7th candidate SARIMA(0,1,3)(2,1,3) starts to show significant pacfs at lag 5 and 6, therefore we stop searching from there downwards. We will keep these three candidates to compare out of sample performance:

	- SARIMA(2,1,3)(1,1,1)
	- SARIMA(2,1,1)(0,1,1)
	- SARIMA(1,1,2)(3,1,3)

-------------------------------------------------------------
Good morning! I just uploaded rmd and pdf of my question 2. To knit the file, pls grab the csv files as well as they store the loop results (so that we don't have to knit the loops again...). 

For 2A), I used the EDA insights to set up 3 search spaces for first, seasonal and both differenced models. Then do the loop, AIC, residuals, out-of-sample routine and settled on SARIMA(2,1,1)(1,0,1). I also specified the model and did a little math to show that the estimated model is stationary after differenced, and invertible.

For 2B), I started with lm(x_t ~ t + m + u) and settled with lm(x_t ~ t + t^2 + m + u). Then the usual CLM routine.

I also wrote answers for the sub-questions about forecast, prediction and model comparisons. It's a little lengthy, but is somewhat thorough with write-up. As with question 1, I hope it summarizes what all three of us have. We can reduce the length later if necessary.


-------------------------------------------------------------
Q.3



tseries::ar ---> find best order using AIC

vars::VARselect --> look for best order using AIC, HQ, SC, FPE
vars::VAR  ---> we specify the order (OLS)
